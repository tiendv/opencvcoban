{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chương 04. Detecting and Tracking Different Body Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong chương này, chúng ta sẽ tìm hiểu cách phát hiện và theo dõi các bộ phận cơ thể khác nhau trong một video livestream. \n",
    "- Chúng ta sẽ bắt đầu bằng cách thảo luận về pileline phát hiện khuôn mặt (face detection pipeline) và cách nó được xây dựng của nó (bottum-up).\n",
    "- Chúng ta sẽ học cách sử dụng framwork này để phát hiện và theo dõi cơ thể khác các bộ phận, chẳng hạn như mắt, tai và miệng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoàn thành phần này, bạn sẽ biết thêm:\n",
    "    + Cách sử dụng Haar cascades để detect các bộ phận trên cơ thể, nhất là face.\n",
    "    + Hình ảnh liên đới là gì ?\n",
    "    + Tăng cường thích ứng là gì ?\n",
    "    + Cách phát hiện và theo dõi khuôn mặt trong luồng video trực tiếp.\n",
    "    + Cách phát hiện và theo dõi mắt trong luồng video trực tiếp.\n",
    "    + Cách tự động phủ kính râm lên trên mặt của một người.\n",
    "    + Cách phát hiện mắt, tai và miệng.\n",
    "    + Làm thế nào để phát hiện học sinh bằng cách sử dụng phân tích hình dạng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nội dung:\n",
    "1. **Using Haar cascades to detect things**\n",
    "2. **What are the interal images ?**\n",
    "3. **Detecting and tracking face**\n",
    "4. **Fun with face**\n",
    "5. **Detecting eyes**\n",
    "6. **Fun with eyes**\n",
    "7. **Detecting ears**\n",
    "8. **Detecting mouth**\n",
    "9. **It's time for a moustache**\n",
    "10. **Detecting pupils**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Haar cascades to detect things ( Sử dụng Haar cascades để phát hiện sự vật)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khi chúng ta nói Haar cascades, chúng ta thực sự đang nói về **phân loại tầng** dựa trên các đặc trưng của Haar. \n",
    "- Để hiểu điều này có nghĩa là gì, chúng ta cần nhìn lại một số thứ và hiểu lý do tại sao chúng ta cần điều này ngay từ đầu. Vào năm 2001, **Paul Viola** và **Michael Jones** đã đưa ra một phương pháp phát hiện đối tượng (object detection) rất hiệu quả trong bài báo chuyên đề của họ. Nó đã trở thành một trong những cột mốc chính trong lĩnh vực học máy (machine learning)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong bài báo của họ, họ đã mô tả một kỹ thuật học máy trong đó một loạt các phân loại đơn giản được sử dụng để có được một phân loại tổng thể thực hiện tốt. Bằng cách này, chúng ta có thể thay đổi quy trình xây dựng một bộ phân loại phức tạp thực hiện với độ chính xác cao. \n",
    "- Điều này rất tuyệt vời là bởi vì việc xây dựng một bộ phân loại chính xác là một quá trình tính toán chuyên sâu. Bên cạnh đó, chúng ta cần rất nhiều dữ liệu huấn luyện để xây dựng một bộ phân loại như vậy. Mô hình cuối trở nên phức tạp và hiệu suất có thể không đạt đến mức."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theo như giảng dạy, có rất nhiều con đường có sẵn. Nhưng tất cả các kỹ thuật truyền thống đều được tính toán chuyên sâu và mô hình thu được sau đó rất phức tạp. Chúng ta không thể sử dụng những mô hình này để xây dựng một hệ thống chạy với thời gian thực (real-time). \n",
    "- Do đó, chúng ta cần giữ cho bộ phân loại đơn giản. Nhưng nếu chúng ta giữ cho bộ phân loại đơn giản, nó sẽ không chính xác. **Sự đánh đổi giữa tốc độ và độ chính xác là phổ biến trong máy học**.\n",
    "- Chúng ta khắc phục vấn đề này bằng cách xây dựng một bộ phân loại đơn giản và sau đó xếp chúng lại với nhau để tạo thành một bộ phân loại thống nhất có độ chính xác cao.\n",
    "- Để đảm bảo rằng quá trình phân loại tổng thể hoạt động tốt, chúng ta cần sáng tạo theo tầng bậc thang. Đây là một trong những lý do chính tại sao phương pháp **Viola-Jones** lại hiệu quả đến vậy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đến với chủ đề phát hiện khuôn mặt (face detection), hãy xem cách huấn luyện một hệ thống để phát hiện khuôn mặt. Nếu chúng ta muốn xây dựng một hệ thống máy học, trước tiên chúng ta cần **trích xuất các đặc trưng (extracting feature)** từ tất cả các hình ảnh. \n",
    "- Ở đây, các thuật toán machine learning sẽ sử dụng các giá trị đặc trưng này để tìm hiểu khuôn mặt trông như thế nào.\n",
    "- Chúng ta sử dụng các đặc trưng Haar để xây dựng các vectơ đặc trưng. Các đặc trưng (features) của Haar là kết hợp của các đặc trưng đơn giản và sự khác biệt của các bản vá trên hình ảnh. Chúng ta **deploy điều này ở nhiều kích cỡ hình ảnh** để đảm bảo hệ thống của chúng ta có **tỷ lệ bất biến (scale-invariant)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu bạn tò mò, bạn có thể tìm hiểu thêm tại: http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khi chúng ta **trích xuất các đặc trưng** này, chúng ta sẽ chuyển qua một loạt các **phân loại (model)**. \n",
    "- Chúng ta chỉ **kiểm tra tất cả các tiểu vùng hình chữ nhật khác nhau** và tiếp tục **loại bỏ những vùng không có mặt** trong đó. Bằng cách này, chúng ta nhanh chóng **đi đến câu trả lời cuối cùng để xem hình chữ nhật đã cho có chứa khuôn mặt hay không**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What are integral images? (Hình ảnh liên đới là gì ? )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Nếu chúng ta muốn tính toán các đặc trưng của Haar, chúng ta sẽ phải tính tổng của nhiều vùng hình chữ nhật khác nhau trong ảnh*.\n",
    "- Nếu chúng ta muốn xây dựng bộ đặc trưng một cách hiệu quả, chúng ta cần tính toán các tổng này theo nhiều tỷ lệ. \n",
    "- **Đây là một quá trình rất tốn kém**! Nếu chúng ta muốn xây dựng một hệ thống thời gian thực, chúng ta không thể dành quá nhiều thời gian để tính toán. Vì vậy, chúng ta sử dụng một cái gì đó gọi là **integral images**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hình ảnh liên đới](images\\integralimage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Để tính tổng của bất kỳ hình chữ nhật nào trong hình ảnh, chúng ta không cần phải đi qua tất cả các phần tử (điểm ảnh) trong khu vực hình chữ nhật đó. Giả sử AP chỉ ra tổng của tất cả các phần tử trong hình chữ nhật được tạo bởi điểm trên cùng bên trái và điểm P trong ảnh là hai góc đối diện chéo nhau. Vì vậy, bây giờ, nếu chúng ta muốn tính diện tích của hình chữ nhật ABCD, chúng ta có thể sử dụng công thức sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Area of the rectangle:  $ABCD = AC - (AB + AD - AA)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tại sao chúng ta quan tâm đến công thức này ? \n",
    "- Như chúng ta đã thảo luận trước đó, Haar **trích xuất các đặc trưng** là bao gồm **tính toán các khu vực của một số lượng lớn hình chữ nhật** trong hình ảnh ở nhiều tỷ lệ. Rất nhiều tính toán đó là lặp đi lặp lại và quá trình là rất chậm nếu xét tổng thể.\n",
    "- Trên thực tế, **nó chậm** đến mức chúng ta **không thể đủ khả năng để chạy bất cứ thứ gì trong thời gian thực (real-time)**. Đó là lý do chúng ta sử dụng công thức này! \n",
    "- Điểm hay của phương pháp này là chúng ta không phải tính toán lại bất cứ điều gì. Tất cả các giá trị cho các khu vực ở phía bên phải của phương trình này đã có sẵn. Vì vậy, chúng ta chỉ sử dụng chúng để tính diện tích của bất kỳ hình chữ nhật nào và trích xuất các đặc trưng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detecting and tracking faces (Phát hiện và theo dõi khuông mặt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OpenCV cung cấp một framework phát hiện khuôn mặt. Chúng ta chỉ cần **tải tập tin cascade** và sử dụng nó để **detect các khuôn mặt trong một hình ảnh**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0)\n",
    "scaling_factor = 1.4\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "    face_rects = face_cascade.detectMultiScale(frame, scaleFactor=1.3, minNeighbors=3)\n",
    "    for (x,y,w,h) in face_rects:\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\n",
    "        cv2.imshow('Face Detector', frame)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu bạn chạy đoạn code trên, kết quả sẽ trông giống như hình ảnh sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding it better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chúng ta cần một mô hình phân loại có thể được sử dụng để phát hiện khuôn mặt trong ảnh. \n",
    "- OpenCV cung cấp một **file XML (`eXtensible Markup Language`)** có thể được sử dụng cho mục đích này. Chúng ta sử dụng hàm **CascadeClassifier** để tải tệp XML. \n",
    "- Tìm hiểu thêm về tập tin XML: https://vi.wikipedia.org/wiki/XML\n",
    "- Khi chúng ta bắt đầu chụp các khung hình đầu vào từ webcam và sử dụng hàm **DetMultiScale** để lấy các **bounding box** cho tất cả các mặt trong hiện tại hình ảnh, trong trường hợp khung không được chuyển qua thang độ xám, điều này sẽ chạy bên trong tại phương pháp, vì các khung màu xám được yêu cầu để xử lý phát hiện. \n",
    "- **Đối số thứ hai** trong hàm này chỉ định bước nhảy trong hệ số tỷ lệ. Nếu chúng ta không tìm thấy hình ảnh trong giới hạn hiện tại, kích thước tiếp theo sẽ được kiểm tra, trong trường hợp của chúng ta, lớn hơn 1,3 lần so với hiện tại kích thước.\n",
    "- **Tham số cuối cùng** là **ngưỡng chỉ định số lượng tối thiểu hình chữ nhật liền kề** cần thiết để giữ hình chữ nhật hiện tại. Nó **có thể được sử dụng để tăng tốc của detector**.\n",
    "- Trong trường hợp mà **nhận dạng khuôn mặt không hoạt động như mong đợi** hãy **giảm ngưỡng giá trị** để có được sự nhận dạng tốt hơn. Trong trường hợp **hình ảnh bị chậm trễ do xử lý phát hiện, giảm kích thước của khung tỷ lệ xuống 0,4 hoặc 0.3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fun with faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta đã biết cách phát hiện và theo dõi khuôn mặt. \n",
    "- Khi chúng ta quay một luồng video từ webcam, chúng ta có thể phủ các mặt nạ ngộ nghĩnh lên trên khuôn mặt của chúng ta. Nó sẽ trông giống như hình ảnh sau đây:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu bạn là người thích mèo, bạn có thể thử cái này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đây là đoạn code phủ mặt nạ siêu nhân lên trên mặt của bạn trong video đầu vào được thu từ wedcam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_frontalface_alt.xml')\n",
    "face_mask = cv2.imread('input\\mask\\green_mask.png')\n",
    "h_mask, w_mask = face_mask.shape[:2]\n",
    "if face_cascade.empty():\n",
    "    raise IOError('Unable to load the face cascade classifier xml file')\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "scaling_factor = 2\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "    face_rects = face_cascade.detectMultiScale(frame, scaleFactor=2, minNeighbors=3)\n",
    "    for (x,y,w,h) in face_rects:\n",
    "        if h <= 0 or w <= 0: pass\n",
    "        # Adjust the height and weight parameters depending on the sizes and the locations.\n",
    "        # You need to play around with these to make sure you get it right.\n",
    "        h, w = int(1.0*h), int(1.0*w)\n",
    "        y -= int(-0.2*h)\n",
    "        x = int(x)\n",
    "        # Extract the region of interest from the image\n",
    "        frame_roi = frame[y:y+h, x:x+w]\n",
    "        face_mask_small = cv2.resize(face_mask, (w, h), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Convert color image to grayscale and threshold it\n",
    "        gray_mask = cv2.cvtColor(face_mask_small, cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask = cv2.threshold(gray_mask, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        # Create an inverse mask\n",
    "        mask_inv = cv2.bitwise_not(mask)\n",
    "        \n",
    "        try: \n",
    "            # Use the mask to extract the face mask region of interest\n",
    "            masked_face = cv2.bitwise_and(face_mask_small, face_mask_small, mask=mask)\n",
    "            # Use the inverse mask to get the remaining part of the image\n",
    "            masked_frame = cv2.bitwise_and(frame_roi, frame_roi, mask=mask_inv)\n",
    "        except cv2.error as e:\n",
    "            print('Ignoring arithmentic exceptions: '+ str(e))\n",
    "                \n",
    "            # add the two images to get the final output\n",
    "        else:\n",
    "            frame[y:y+h, x:x+w] = cv2.add(masked_face, masked_frame)\n",
    "            \n",
    "    cv2.imshow('Face Detector', frame)\n",
    "        \n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cũng giống như trước đây, trước tiên chúng ta **tải tệp XML phân loại face cascade**. Các bước **detect khuôn mặt làm như bình thường**. \n",
    "- Chúng ta bắt đầu vòng lặp vô hạn và tiếp tục detect khuôn mặt trong mỗi khung hình. Một lần chúng ta biết khuôn mặt ở đâu, chúng ta cần sửa đổi tọa độ một chút để đảm bảo mặt nạ phù hợp đúng cách. Quá trình thao tác này là chủ quan và phụ thuộc vào mặt nạ bạn chọn. Mặt nạ khác nhau đòi hỏi mức độ điều chỉnh khác nhau để làm cho nó trông tự nhiên hơn. Chúng ta trích xuất vùng quan tâm từ khung đầu vào trong dòng sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_roi = frame[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta có khu vực quan tâm (cái mặt của mình á), chúng ta cần phủ mặt nạ lên khu vực này.\n",
    "- Vì vậy, chúng ta thay đổi kích thước mặt nạ đầu vào để đảm bảo nó phù hợp với khu vực quan tâm này.\n",
    "- Đầu vào mặt nạ có nền trắng. Vì vậy, nếu chúng ta chỉ phủ lớp này lên trên khu vực quan tâm, thì nó sẽ trông không tự nhiên vì nền trắng. Chúng ta chỉ cần phủ các pixel mặt mèo; diện tích còn lại phải trong suốt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vì vậy, trong bước tiếp theo, chúng ta tạo ra một mặt nạ bằng cách  hình ảnh mèo. Kể từ khi nền là màu trắng, chúng tôi ngưỡng hình ảnh sao cho bất kỳ pixel nào có giá trị cường độ lớn hơn 180 trở thành số không và mọi thứ khác trở thành 255. Theo như khu vực khung quan tâm là quan tâm, chúng ta cần bôi đen mọi thứ trong khu vực mặt nạ này. Chúng ta có thể làm điều đó bằng cách đơn giản sử dụng nghịch đảo của mặt nạ mà chúng ta vừa tạo. Khi chúng ta có các phiên bản đeo mặt nạ của hình ảnh mặt mèo và khu vực đầu vào quan tâm, chúng ta chỉ cần thêm chúng để có được hình ảnh cuối cùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the alpha channel from the overlay image (Loại bỏ các kênh alpha từ hình ảnh lớp phủ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do sử dụng hình ảnh lớp phủ, chúng ta nên nhớ rằng một lớp có thể được xây dựng trên các pixel đen, điều này sẽ tạo ra hiệu ứng không mong muốn đối với kết quả của mã của chúng ta.\n",
    "- Nhằm tránh vấn đề đó, đoạn code sau sẽ loại bỏ lớp kênh alpha khỏi hình ảnh lớp phủ của bạn và do đó cho phép chúng ta thu được kết quả tốt trên các đoạn code mẫu mà chúng ta có trong chương này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "def remove_alpha_channel(source, background_color):\n",
    "    source_img = cv2.cvtColor(source[:,:,:3], cv2.COLOR_BGR2GRAY)\n",
    "    source_mask = source[:,:,3] * (1 / 255.0)\n",
    "    bg_part = (255 * (1 / 255.0)) * (1.0 - source_mask)\n",
    "    weight = (source_img * (1 / 255.0)) * (source_mask)\n",
    "    dest = np.uint8(cv2.addWeighted(bg_part, 255.0, weight, 255.0, 0.0))\n",
    "    return dest\n",
    "\n",
    "orig_img = cv2.imread('input\\mask\\lion_mask.png', cv2.IMREAD_UNCHANGED)\n",
    "dest_img = remove_alpha_channel(orig_img)\n",
    "cv2.imwrite('output\\overlay_dest.png', dest_img, [cv2.IMWRITE_PNG_COMPRESSION])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detecting eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta đã hiểu cách phát hiện khuôn mặt, chúng ta có thể khái quát khái niệm để phát hiện khuôn mặt. Điều này cũng tương tự trong việc phát hiện các bộ phận của khuông mặt.\n",
    "- Điều quan trọng là phải hiểu rằng framework Viola-Jones có thể được áp dụng cho bất kỳ đối tượng. Độ chính xác và tốc độ sẽ phụ thuộc vào tính đặc biệt, điển hình của đối tượng.\n",
    "- Ví dụ, khuôn mặt người có những đặc điểm rất độc đáo, vì vậy thật dễ dàng để training hệ thống trở nhanh chóng. Mặt khác, các đồ vật như khăn, quần áo hoặc sách quá chung chung, và có không có đặc điểm phân biệt như vậy, vì vậy khó khăn hơn để xây dựng một máy dò mạnh mẽ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chúng ta hãy xem cách xây dựng một eye detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_frontalface_alt.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_eye.xml')\n",
    "if face_cascade.empty():\n",
    "    raise IOError('Unable to load the face cascade classifier xml file')\n",
    "if eye_cascade.empty():\n",
    "    raise IOError('Unable to load the eye cascade classifier xml file')\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "ds_factor = 0.5\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=ds_factor, fy=ds_factor, interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=1)\n",
    "    for (x,y,w,h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (x_eye,y_eye,w_eye,h_eye) in eyes:\n",
    "            center = (int(x_eye + 0.5*w_eye), int(y_eye + 0.5*h_eye))\n",
    "            radius = int(0.3 * (w_eye + h_eye))\n",
    "            color = (0, 255, 0)\n",
    "            thickness = 3\n",
    "            cv2.circle(roi_color, center, radius, color, thickness)\n",
    "        \n",
    "    cv2.imshow('Eye Detector', frame)\n",
    "    \n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu bạn chạy chương trình này, kết quả sẽ tựa tựa như vầy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afterthought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu bạn để ý, chương trình này trông rất giống với chương trình nhận diện khuôn mặt. \n",
    "- Cùng với việc download the face detection cascade classifier, chúng ta the eye detection cascade classifier.\n",
    "- Về mặt kỹ thuật, chúng ta không cần sử dụng máy face detector. Nhưng chúng ta biết rằng đôi mắt luôn ở trên khuôn mặt của ai đó. Chúng ta sử dụng thông tin này và tìm kiếm mắt chỉ trong khu vực quan tâm có liên quan, đó là khuôn mặt.\n",
    "- Đầu tiên chúng ta detect khuôn mặt, và sau đó thực hiện thuật toán detecting eye trên hình ảnh phụ này. Bằng cách này, nó nhanh hơn và hiệu quả hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fun with eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta đã biết cách phát hiện mắt trong một hình ảnh, hãy xem liệu chúng ta có thể làm điều gì đó thú vị với nó không. Chúng ta có thể làm một cái gì đó giống như những gì được hiển thị trong ảnh chụp màn hình sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đây là đoạn code giúp ta thực hiện được điều bên trên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_frontalface_alt.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_eye.xml')\n",
    "\n",
    "if face_cascade.empty():\n",
    "    raise IOError('Unable to load the face cascade classifier xml file')\n",
    "if eye_cascade.empty():\n",
    "    raise IOError('Unable to load the eye cascade classifier xml file')\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "sunglasses_img = cv2.imread('input\\mask\\sunglasses.png')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "    vh, vw = frame.shape[:2]\n",
    "    vh, vw = int(vh), int(vw)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=1)\n",
    "    \n",
    "    centers = []\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (x_eye,y_eye,w_eye,h_eye) in eyes:\n",
    "            centers.append((x + int(x_eye + 0.5*w_eye), y + int(y_eye + 0.5*h_eye)))\n",
    "    if len(centers) > 1: # if detects both eyes\n",
    "        h, w = sunglasses_img.shape[:2]\n",
    "        # Extract the region of interest from the image\n",
    "        eye_distance = abs(centers[1][0] - centers[0][0])\n",
    "        # Overlay sunglasses; the factor 2.12 is customizable depending on the size of the face\n",
    "        sunglasses_width = 2.12 * eye_distance\n",
    "        scaling_factor = sunglasses_width / w\n",
    "        print(scaling_factor, eye_distance)\n",
    "        overlay_sunglasses = cv2.resize(sunglasses_img, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        x = centers[0][0] if centers[0][0] < centers[1][0] else centers[1][0]\n",
    "        # customizable X and Y locations; depends on the size of the face\n",
    "        x -= int(0.26*overlay_sunglasses.shape[1])\n",
    "        y += int(0.26*overlay_sunglasses.shape[0])\n",
    "        h, w = overlay_sunglasses.shape[:2]\n",
    "        h, w = int(h), int(w)\n",
    "        frame_roi = frame[y:y+h, x:x+w]\n",
    "        # Convert color image to grayscale and threshold it\n",
    "        gray_overlay_sunglassess = cv2.cvtColor(overlay_sunglasses, cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask = cv2.threshold(gray_overlay_sunglassess, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        # Create an inverse mask\n",
    "        mask_inv = cv2.bitwise_not(mask)\n",
    "        \n",
    "        try:\n",
    "            # Use the mask to extract the face mask region of interest\n",
    "            masked_face = cv2.bitwise_and(overlay_sunglasses,\n",
    "            overlay_sunglasses, mask=mask)\n",
    "            # Use the inverse mask to get the remaining part of the image\n",
    "            masked_frame = cv2.bitwise_and(frame_roi, frame_roi, mask=mask_inv)\n",
    "        except cv2.error as e:\n",
    "            print('Ignoring arithmentic exceptions: '+ str(e))\n",
    "            #raise e\n",
    "            \n",
    "        # add the two images to get the final output\n",
    "        frame[y:y+h, x:x+w] = cv2.add(masked_face, masked_frame)\n",
    "    else:\n",
    "        print('Eyes not detected')\n",
    "        \n",
    "    cv2.imshow('Eye Detector', frame)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positioning the sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Giống như chúng ta đã làm trước đó, chúng ta tải hình ảnh và phát hiện mắt. Một khi chúng tôi phát hiện ra đôi mắt, chúng ta thay đổi kích thước hình ảnh kính râm để phù hợp với khu vực quan tâm hiện tại. \n",
    "- Tạo vùng quan tâm, chúng ta xem xét khoảng cách giữa hai mắt. Chúng ta thay đổi kích thước hình ảnh cho phù hợp và sau đó tiếp tục và tạo một mặt nạ. Điều này tương tự như những gì chúng ta đã làm với mặt nạ mèo trước đó. \n",
    "- Vị trí của kính râm trên mặt là chủ quan, vì vậy bạn sẽ phải tìm hiểu trọng lượng nếu bạn muốn sử dụng một cặp kính râm khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detecting ears"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Một lần nữa, thông qua việc sử dụng các tệp phân loại cascade Haar, đoạn code dưới đây sẽ xác định mỗi tai, làm nổi bật chúng một khi chúng được phát hiện.\n",
    "- Như bạn có thể nhận thấy, hai bộ phân loại khác nhau được tạo ra vì tọa độ cho mỗi tai khác nhau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to load the left ear cascade classifier xml file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d3b72d357cca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mleft_ear_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unable to load the left ear cascade classifier xml file'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mright_ear_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load the left ear cascade classifier xml file"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "left_ear_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_mcs_leftear.xml')\n",
    "right_ear_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_mcs_rightear.xml')\n",
    "\n",
    "if left_ear_cascade.empty():\n",
    "    raise IOError('Unable to load the left ear cascade classifier xml file')\n",
    "    \n",
    "if right_ear_cascade.empty():\n",
    "    raise IOError('Unable to load the right ear cascade classifier xml file')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "scaling_factor = 0.5\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    left_ear = left_ear_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=3)\n",
    "    right_ear = right_ear_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=3)\n",
    "    \n",
    "    for (x,y,w,h) in left_ear: \n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\n",
    "    \n",
    "    for (x,y,w,h) in right_ear:\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 3)\n",
    "    \n",
    "    cv2.imshow('Ear Detector', frame)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detecting a mouth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lần này, sử dụng phân loại Haar, chúng ta sẽ trích xuất một vị trí miệng từ đầu vào dòng video, và trên mã bên dưới mã này chúng ta sẽ sử dụng những tọa độ để đặt một bộ ria mép trên khuôn mặt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<class 'cv2.CascadeClassifier'> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.1) C:\\projects\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:567: error: (-2:Unspecified error) in function 'bool __cdecl cv::HaarEvaluator::Feature::read(const class cv::FileNode &,const class cv::Size_<int> &)'\n> Invalid HAAR feature (expected: 'rw.r.x < W'), where\n>     'rw.r.x' is 17\n> must be less than\n>     'W' is 15\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a69d1d6577bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmouth_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haar-cascade-files\\haarcascade_mcs_mouth.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmouth_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <class 'cv2.CascadeClassifier'> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mouth_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_mcs_mouth.xml')\n",
    "\n",
    "if mouth_cascade.empty():\n",
    "    raise IOError('Unable to load the mouth cascade classifier xml file')\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "ds_factor = 1\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=ds_factor, fy=ds_factor, interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    mouth_rects = mouth_cascade.detectMultiScale(gray, scaleFactor=1.7, minNeighbors=11)\n",
    "    for (x,y,w,h) in mouth_rects:\n",
    "        y = int(y - 0.15*h)\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\n",
    "        break\n",
    "        \n",
    "    cv2.imshow('Mouth Detector', frame)\n",
    "    \n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hình ảnh đầu ra trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. It's time for a moustache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hãy để một bộ ria mép trên miệng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<class 'cv2.CascadeClassifier'> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.1) C:\\projects\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:567: error: (-2:Unspecified error) in function 'bool __cdecl cv::HaarEvaluator::Feature::read(const class cv::FileNode &,const class cv::Size_<int> &)'\n> Invalid HAAR feature (expected: 'rw.r.x < W'), where\n>     'rw.r.x' is 17\n> must be less than\n>     'W' is 15\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-618049138b86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmouth_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haar-cascade-files\\haarcascade_mcs_mouth.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmoustache_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'moustache.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <class 'cv2.CascadeClassifier'> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mouth_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_mcs_mouth.xml')\n",
    "\n",
    "moustache_mask = cv2.imread('moustache.png')\n",
    "h_mask, w_mask = moustache_mask.shape[:2]\n",
    "\n",
    "if mouth_cascade.empty():\n",
    "    raise IOError('Unable to load the mouth cascade classifier xml file')\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "scaling_factor = 1\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    mouth_rects = mouth_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    if len(mouth_rects) > 0:\n",
    "        (x,y,w,h) = mouth_rects[0]\n",
    "        h, w = int(0.6*h), int(1.2*w)\n",
    "        x -= int(0.05*w)\n",
    "        y -= int(0.55*h)\n",
    "        frame_roi = frame[y:y+h, x:x+w]\n",
    "        moustache_mask_small = cv2.resize(moustache_mask, (w, h), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        gray_mask = cv2.cvtColor(moustache_mask_small, cv2.COLOR_BGR2GRAY)\n",
    "        ret, mask = cv2.threshold(gray_mask, 50, 255, cv2.THRESH_BINARY_INV)\n",
    "        mask_inv = cv2.bitwise_not(mask)\n",
    "        masked_mouth = cv2.bitwise_and(moustache_mask_small, moustache_mask_small, mask=mask)\n",
    "        masked_frame = cv2.bitwise_and(frame_roi, frame_roi, mask=mask_inv)\n",
    "        frame[y:y+h, x:x+w] = cv2.add(masked_mouth, masked_frame)\n",
    "        \n",
    "    cv2.imshow('Moustache', frame)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output sẽ có dạng như hình dưới đây:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detecting Pupils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chúng ta sẽ có một cách tiếp cận khác nhau ở đây. Học sinh quá chung chung để thực hiện phương pháp thác Haar. Chúng ta cũng sẽ tuân theo trên nguyên tắc phát hiện mọi thứ dựa trên hình dạng của chúng. Sau đây là những gì đầu ra sẽ như thế nào:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import cv2\n",
    "\n",
    "eye_cascade = cv2.CascadeClassifier('haar-cascade-files\\haarcascade_eye.xml')\n",
    "\n",
    "if eye_cascade.empty():\n",
    "    raise IOError('Unable to load the eye cascade classifier xml file')\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "ds_factor = 1\n",
    "ret, frame = cap.read()\n",
    "contours = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, None, fx=ds_factor, fy=ds_factor, interpolation=cv2.INTER_AREA)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=1)\n",
    "    for (x_eye, y_eye, w_eye, h_eye) in eyes:\n",
    "        pupil_frame = gray[y_eye:y_eye + h_eye, x_eye:x_eye + w_eye]\n",
    "        ret, thresh = cv2.threshold(pupil_frame, 80, 255, cv2.THRESH_BINARY)\n",
    "        cv2.imshow(\"threshold\", thresh)\n",
    "        im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        print(contours)\n",
    "        \n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            rect = cv2.boundingRect(contour)\n",
    "            x, y, w, h = rect\n",
    "            radius = 0.15 * (w + h)\n",
    "            \n",
    "            area_condition = (100 <= area <= 200)\n",
    "            symmetry_condition = (abs(1 - float(w)/float(h)) <= 0.2) \n",
    "            fill_condition = (abs(1 - (area / (math.pi * math.pow(radius, 2.0)))) <= 0.4)\n",
    "            cv2.circle(frame, (int(x_eye + x + radius), int(y_eye + y + radius)),int(1.3 * radius), (0, 180, 0), -1)\n",
    "            \n",
    "    cv2.imshow('Pupil Detector', frame)\n",
    "    c = cv2.waitKey(1)\n",
    "    if c == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconstructing the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Như chúng ta đã thảo luận trước đó, chúng ta sẽ không sử dụng Haar để phát hiện pupil. Nếu chúng ta không thể sử dụng một bộ phân loại được training trước, thì chúng ta sẽ phát hiện ra các pipils như thế nào? \n",
    "- Chúng ta có thể sử dụng phân tích hình dạng để phát hiện các học sinh. Chúng ta biết rằng pupils là hình tròn, vì vậy chúng ta có thể sử dụng thông tin này để phát hiện chúng trong hình ảnh. \n",
    "- Chúng ta đảo ngược hình ảnh đầu vào và sau đó chuyển đổi nó thành một hình ảnh thang độ xám như trong dòng sau: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Như chúng ta có thể thấy ở đây, chúng ta có thể **đảo ngược một hình ảnh** bằng cách sử dụng **toán tử dấu ngã (tidle operator)**.\n",
    "- **Đảo ngược hình ảnh** rất hữu ích trong trường hợp của chúng ta vì con ngươi có màu đen và màu đen tương ứng với giá trị pixel thấp. Sau đó, chúng ta đặt ngưỡng hình ảnh để đảm bảo rằng chỉ có các pixel đen và trắng.\n",
    "- Bây giờ, chúng ta phải tìm ra ranh giới của tất cả các hình dạng. OpenCV cung cấp một chức năng tốt để đạt được điều này, đó là **findContours**. Chúng ta sẽ thảo luận thêm về điều này trong các chương sắp tới. **Nhưng bây giờ, tất cả những gì chúng ta cần biết là hàm này trả về tập hợp các ranh giới của tất cả các hình dạng được tìm thấy trong hình ảnh.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bước tiếp theo là xác định hình dạng của con ngươi và loại bỏ phần còn lại. Chúng ta sẽ sử dụng các thuộc tính nhất định của vòng tròn để zero-in trên hình này.\n",
    "- Hãy xem xét tỷ lệ chiều rộng và chiều cao của hình chữ nhật giới hạn. Nếu hình dạng là một hình tròn, tỷ lệ này sẽ là một. Chúng ta có thể sử dụng hàm ràng buộc để có được tọa độ của hình chữ nhật giới hạn. Hãy xem xét diện tích của hình dạng này. Nếu chúng ta tính toán gần đúng bán kính của hình này và sử dụng công thức cho diện tích hình tròn, thì nó sẽ gần với diện tích của đường viền này. \n",
    "- Chúng ta có thể sử dụng hàm **contourArea** để tính diện tích của bất kỳ đường viền nào trong ảnh. Vì vậy, chúng ta có thể sử dụng các điều kiện này và lọc ra các hình dạng.\n",
    "- Sau khi chúng tôi làm điều đó, chúng tôi còn lại hai học sinh trong hình ảnh. Chúng ta có thể tinh chỉnh nó hơn nữa bằng cách giới hạn vùng tìm kiếm trên khuôn mặt hoặc mắt.\n",
    "- Vì bạn biết cách phát hiện khuôn mặt và mắt, bạn có thể dùng thử và xem liệu bạn có thể làm cho nó hoạt động cho một luồng video trực tiếp hay không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu bạn cảm thấy muốn nghịch với một loại phát hiện cơ thể khác, chỉ cần truy cập đường link sau để tìm các phân loại bộ phận cở thể khác: https://github.com/opencv/opencv/tree/master/data/haarcascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong chương này, chúng ta đã thảo luận về thác Haar và cách tích hợp hình ảnh. \n",
    "- Chúng ta hiểu làm thế nào các hệ thống phát hiện khuôn mặt được xây dựng.\n",
    "- Chúng ta đã học cách phát hiện và theo dõi khuôn mặt trong một luồng video trực tiếp.\n",
    "- Chúng ta đã thảo luận cách sử dụng hệ thống phát hiện khuôn mặt để phát hiện các bộ phận cơ thể khác nhau, chẳng hạn như mắt, tai, mũi và miệng.\n",
    "- Chúng ta đã học cách phủ mặt nạ lên trên hình ảnh bằng cách sử dụng kết quả phát hiện các bộ phận cơ thể. Chúng ta đã sử dụng các nguyên tắc phân tích hình dạng để phát hiện các pupils.\n",
    "- **Trong chương tiếp theo, chúng ta sẽ thảo luận về phát hiện đặc trưng và cách sử dụng nó để hiểu nội dung hình ảnh**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đọc thêm về *Threshold (ngưỡng)*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nguỡng (Threshold) là một khái niệm khá quen thuộc trong xử lý ảnh cũng như rất nhiều giải thuật khác. Nó dùng để chỉ một giá trị mà người ta dựa vào để phân hoạch một tập hợp thành các miền phân biệt.\n",
    "- Ví dụ thang điểm đánh giá học sinh là từ 1 đến 10. Trong một tập hợp gồm 40 học sinh của 1 lớp, người ta muốn phân lọai ra hai miền, miền thứ nhất bao gồm các học sinh đạt yêu cầu và miền thứ hai gồm các học sinh không đạt. Trong tình huống đó người ta dùng giá trị 5 (điểm) như là một ngưỡng (threshold) để phân loại học sinh. Các học sinh có điểm dưới 5 sẽ xem như không đạt, những học sinh có điểm từ 5 trở lên là đạt yêu cầu. Giá trị ngưỡng thường được xác định dựa vào những điểm đặc biệt (ví dụ ở trung bình), dựa vào kinh nghiệm khảo sát. Nếu dựa vào số lượng Ngưỡng áp dụng cho cùng một tập dữ liệu người ta sẽ phân ra các phương pháp ứng dụng ngưỡng đơn, ngưỡng kép, hay đa ngưỡng. Nếu dựa vào sự biến thiên của giá trị Ngưỡng, trong cùng phạm vi ứng dụng người ta sẽ phân ra các phương pháp dùng ngưỡng cố định (Constant|Fixed Threshold) và không cố định (Adaptive Threshold). Ngưỡng không cố định nghĩa là giá trị của nó sẽ thay đổi tùy theo sự biến thiên của tập dử liệu theo không gian và thời gian. Thông thường giá trị này được xác định thông qua khảo sát tập dử liệu bằng phương pháp thống kê. Để dễ hình dung hơn về ứng dụng khái niệm Threshold, sau đây chúng ta sẻ xét một ví dụ bộ lọc ngưỡng (Threshold Filter) đơn giản trong xử lý ảnh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Threshold](images\\threshold1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Với mỗi pixel trong hình đa mức xám (grayscale) ở trên giá trị sẽ trong khoảng 0 - 255 vậy pixel nào lớn hơn ngưỡng là 120 ta gán giá trị cho nó thành đen (0), ngược lại gán giá trị trắng (255). Kết quả thu được như sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Threshold](images\\threshold2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
