{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chương 10. Augimented Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong chương này, bạn sẽ tìm hiểu về thực tế tăng cường và cách bạn có thể sử dụng nó để xây dựng các ứng dụng tuyệt vời. Chúng ta sẽ thảo luận về ước tính tư thế và theo dõi máy bay. Bạn sẽ tìm hiểu cách ánh xạ tọa độ từ 3D sang 2D và cách chúng ta có thể phủ đồ họa lên trên một video đang trực tiếp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đến cuối chương này, bạn sẽ biết:\n",
    "- Tiền đề của thực tế tăng cường\n",
    "- Ước tính tư thế là gì\n",
    "- Làm thế nào để theo dõi một đối tượng phẳng\n",
    "- Cách ánh xạ tọa độ từ 3D sang 2D\n",
    "- Cách phủ lớp đồ họa lên trên video trong thời gian thực"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nội dung:\n",
    "1. **What is the premise of augmented reality?**\n",
    "2. **What does an augmented reality system look like?**\n",
    "3. **Geometric transformations for augmented reality**\n",
    "4. **What is pose estimation?**\n",
    "5. **How to track planar objects**\n",
    "6. **How to augment our reality**\n",
    "    + 6.1 Mapping coordinates from 3D to 2D\n",
    "    + 6.2 How to overlay 3D objects on a video\n",
    "    + 6.3 Let's look at the code\n",
    "7. **Let's add some movements**\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the premise of augmented reality ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trước khi chúng ta nhảy vào tất cả những thứ thú vị, hãy hiểu **thực tế tăng cường (augmented reality)** có nghĩa là gì. \n",
    "- Bạn có thể đã thấy thuật ngữ thực tế tăng cường được sử dụng trong nhiều ngữ cảnh. Vì vậy, chúng ta nên hiểu tiền đề của thực tế tăng cường trước khi chúng ta bắt đầu thảo luận về các chi tiết thực hiện.\n",
    "- Thực tế tăng cường (Augemented reality) đề cập đến **sự chồng chất của đầu vào được tạo ra từ máy tính**, chẳng hạn như hình ảnh, âm thanh, đồ họa và văn bản, trên đỉnh của thế giới thực."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thực tế tăng cường (Augmented Reality) cố gắng **làm mờ ranh giới giữa những gì thực và những gì máy tính tạo ra** bằng cách **kết hợp thông tin liền mạch và tăng cường những gì chúng ta thấy và cảm nhận**.\n",
    "- Nó thực sự liên quan chặt chẽ đến một khái niệm gọi là **mediated reality**, trong đó một máy tính điều chỉnh view của chúng ta về thực tế.\n",
    "- Do đó, công nghệ này hoạt động bằng cách nâng cao nhận thức hiện tại của chúng ta về thực tế. Bây giờ, thách thức ở đây là làm cho nó trông liền mạch với người dùng.\n",
    "- Thật dễ dàng để chỉ chồng một cái gì đó lên trên video đầu vào, nhưng chúng ta cần làm cho nó trông như thể nó là một phần của video. Người dùng nên cảm thấy rằng đầu vào do máy tính phản ánh chặt chẽ thế giới thực. Đây là những gì chúng ta muốn đạt được khi chúng ta xây dựng một hệ thống thực tế tăng cường (Augmented Reality System)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Những nghiên cứu thị giác máy tính trong lĩnh vực này tìm ra cách chúng ta có thể áp dụng hình ảnh do máy tính tạo ra cho các luồng video trực tiếp để chúng ta có thể nâng cao nhận thức về thế giới thực.\n",
    "- Công nghệ thực tế tăng cường (Augmented Reality) có rất nhiều ứng dụng, bao gồm: xe tự động, trực quan hóa dữ liệu, chơi game, xây dựng, so on. \n",
    "- Bây giờ chúng ta có điện thoại thông minh mạnh và máy móc thông minh hơn, chúng ta có thể xây dựng các ứng dụng thực tế tăng cường cao cấp một cách dễ dàng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What does an augmented reality system look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quan sát bức ảnh dưới đây:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Augmentation System](images\\augmentation-system.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Như chúng ta có thể thấy ở đây, máy ảnh quay video thế giới thực để lấy điểm tham chiếu (reference points). Hệ thống đồ họa tạo ra các đối tượng ảo cần được phủ lên trên video.\n",
    "- Bây giờ, khối hợp nhất video là Video Merging là nơi tất cả các \"phép thuật\" xảy ra. Khối này phải đủ thông minh để hiểu cách phủ các vật thể ảo lên trên thế giới thực theo cách tốt nhất có thể."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geometric transformations for augmented reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kết quả của thực tế tăng cường là tuyệt vời, nhưng có rất nhiều điều toán học đang diễn ra bên dưới (under the hood).\n",
    "- Thực tế tăng cường sử dụng rất nhiều phép biến đổi hình học và các hàm toán học liên quan để đảm bảo mọi thứ đều trơn tru.\n",
    "- Khi nói về một video trực tiếp cho thực tế tăng cường, chúng ta cần đăng ký chính xác các đối tượng ảo trên đỉnh của thế giới thực. Để hiểu rõ hơn về điều này, chúng ta hãy nghĩ về nó như một sự liên kết của hai máy ảnh: máy ảnh thực mà chúng ta nhìn thế giới và máy ảo chiếu các đối tượng đồ họa do máy tính tạo ra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Để xây dựng một hệ thống thực tế tăng cường, các biến đổi hình học sau đây cần được thiết lập:\n",
    "    + **Object-to-Scene**: Chuyển đổi này đề cập đến việc chuyển đổi tọa độ 3D của một đối tượng ảo và thể hiện chúng trong khung tọa độ của cảnh trong thế giới thực của chúng ta. Điều này đảm bảo rằng chúng ta đang đặt đối tượng ảo vào đúng vị trí.\n",
    "    + **Scene-to-camera**: Chuyển đổi này đề cập đến tư thế của máy ảnh trong thế giới thực. Theo tư thế, chúng ta có nghĩa là định hướng và vị trí của máy ảnh. Chúng ta cần ước tính điểm nhìn của camera để biết cách che phủ đối tượng ảo.\n",
    "    + **Camera-to-image**: Điều này đề cập đến các thông số hiệu chuẩn của máy ảnh. Điều này xác định cách chúng ta có thể chiếu một đối tượng 3D lên mặt phẳng hình ảnh 2D. Đây là hình ảnh mà chúng ta thực sự sẽ thấy cuối cùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quan sát hình ảnh sau đây:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ví dụ](images\\example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Như chúng ta có thể thấy ở đây, chiếc xe đang `cố gắng hòa hợp` với khung cảnh nhưng trông rất giả tạo.\n",
    "- Nếu chúng ta không chuyển đổi tọa độ theo đúng cách, chiếc xe sẽ trông không tự nhiên.\n",
    "- Đây là những gì chúng ta đã nói về việc chuyển đổi Object-to-Scene! Khi chúng ta chuyển đổi tọa độ 3D của đối tượng ảo thành khung tọa độ của thế giới thực, chúng ta cần ước tính tư thế của máy ảnh:\n",
    "\n",
    "![Example](images\\example1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chúng ta cần hiểu vị trí và góc quay của camera vì đó là những gì người dùng sẽ thấy. Khi chúng ta ước tính tư thế máy ảnh, chúng ta sẵn sàng đưa cảnh 3D này lên hình ảnh 2D:\n",
    "\n",
    "![Example](images\\example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khi chúng ta có những biến đổi này, chúng ta có thể xây dựng hệ thống hoàn chỉnh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What is pose estimation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trước khi tiến hành, chúng ta cần hiểu cách ước tính camera pose(tư thế máy ảnh). Đây là một bước rất quan trọng trong một hệ thống thực tế tăng cường và chúng ta cần phải làm cho đúng nếu chúng ta muốn trải nghiệm của mình được liền mạch.\n",
    "- Trong thế giới của thực tế tăng cường, chúng ta phủ lớp đồ họa lên trên một vật thể trong thời gian thực. Để làm được điều đó, chúng ta cần biết vị trí và hướng của camera, và chúng ta cần thực hiện nhanh chóng.\n",
    "- Đây là nguyên nhân vì sao **pose estimation (ước tính tư thế)** trở nên rất quan trọng. Nếu bạn không theo dõi tư thế một cách chính xác, đồ họa bị che khuất sẽ trông không tự nhiên.\n",
    "- Hãy xem xét hình ảnh sau đây:\n",
    "\n",
    "![](images\\example3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mũi tên chỉ ra rằng bề mặt là bình thường. Giả sử đối tượng thay đổi hướng của nó:\n",
    "\n",
    "![](images\\example4.png)\n",
    "- Bây giờ, mặc dù vị trí là như nhau, định hướng đã thay đổi. Chúng ta cần có thông tin này để đồ họa phủ lên trông tự nhiên. Chúng ta cần đảm bảo rằng đồ họa được căn chỉnh theo hướng và vị trí này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How to track planar objects (Cách theo dõi đối tượng phẳng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ bạn đã hiểu ước tính tư thế (pose estimation) là gì, hãy xem cách bạn có thể sử dụng nó để theo dõi các đối tượng phẳng. Hãy xem xét các đối tượng phẳng sau:\n",
    "\n",
    "![](images\\example5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ, nếu chúng ta trích xuất các điểm đặc trưng từ hình ảnh này bằng SIFT, chúng ta sẽ thấy một cái gì đó như thế này:\n",
    "\n",
    "![](images\\example6.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hãy nghiêng hộp các tông:\n",
    "\n",
    "![](images\\example7.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Như chúng ta có thể thấy, hộp các tông bị nghiêng trong hình ảnh này. Bây giờ, nếu chúng ta muốn đảm bảo đối tượng ảo của chúng ta được phủ lên trên bề mặt này, chúng ta cần thu thập thông tin mặt phẳng nghiên này.\n",
    "- Một cách để làm điều này là sử dụng các vị trí tương đối của các điểm đặc trưng. Nếu chúng ta trích xuất các điểm đặc trưng từ hình ảnh trên, nó sẽ trông như thế này:\n",
    "\n",
    "![](images\\example8.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Như bạn có thể thấy, các điểm đặc trưng tiến gần hơn theo chiều ngang ở phía xa của mặt phẳng so với các điểm ở gần cuối:\n",
    "\n",
    "![](images\\example9.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vì vậy, chúng ta có thể sử dụng thông tin này để trích xuất thông tin định hướng từ hình ảnh. Nếu bạn còn nhớ, chúng ta đã thảo luận chi tiết về chuyển đổi phối cảnh khi chúng ta học về các biến đổi hình học, cũng như hình ảnh toàn cảnh. Tất cả những gì chúng ta cần làm là sử dụng hai tập hợp điểm đó và trích xuất ma trận `homography`. **Ma trận homography này sẽ cho chúng ta biết làm thế nào các hộp các tông di chuyển**.\n",
    "\n",
    "- Hãy quan sát hình ảnh dưới đây:\n",
    "\n",
    "![](images\\example10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trước tiên, chúng ta sẽ bắt đầu bằng cách chọn vùng quan tâm bằng cách sử dụng lớp **ROISelector** và sau khi chúng ta thực hiện điều đó, chúng ta sẽ chuyển các tọa độ đó cho **PoseEstimator**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROISelector(object):\n",
    "    def __init__(self, win_name, init_frame, callback_func):\n",
    "        self.callback_func = callback_func\n",
    "        self.selected_rect = None\n",
    "        self.drag_start = None\n",
    "        self.tracking_state = 0\n",
    "        event_params = {\"frame\": init_frame}\n",
    "        cv2.namedWindow(win_name)\n",
    "        cv2.setMouseCallback(win_name, self.mouse_event, event_params)\n",
    "        \n",
    "    def mouse_event(self, event, x, y, flags, param):\n",
    "        x, y = np.int16([x, y])\n",
    "        \n",
    "        # Detecting the mouse button down event\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.drag_start = (x, y)\n",
    "            self.tracking_state = 0\n",
    "            \n",
    "        if self.drag_start:\n",
    "            if event == cv2.EVENT_MOUSEMOVE:\n",
    "                h, w = param[\"frame\"].shape[:2]\n",
    "                xo, yo = self.drag_start\n",
    "                x0, y0 = np.maximum(0, np.minimum([xo, yo], [x, y]))\n",
    "                x1, y1 = np.minimum([w, h], np.maximum([xo, yo], [x, y]))\n",
    "                self.selected_rect = None\n",
    "                \n",
    "                if x1-x0 > 0 and y1-y0 > 0:\n",
    "                    self.selected_rect = (x0, y0, x1, y1)\n",
    "            elif event == cv2.EVENT_LBUTTONUP:\n",
    "                self.drag_start = None\n",
    "            if self.selected_rect is not None:\n",
    "                self.callback_func(self.selected_rect)\n",
    "                self.selected_rect = None\n",
    "                self.tracking_state = 1\n",
    "                \n",
    "    def draw_rect(self, img, rect):\n",
    "        if not rect: return False\n",
    "        x_start, y_start, x_end, y_end = rect\n",
    "        cv2.rectangle(img, (x_start, y_start), (x_end, y_end), (0, 255, 0),2)\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong hình ảnh sau đây, vùng quan tâm của hình chữ nhật màu xanh lá cây:\n",
    "![](images\\example11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau đó chúng ta trích xuất các điểm đặc trưng từ khu vực quan tâm này. Vì chúng ta đang theo dõi các đối tượng phẳng, thuật toán giả định rằng vùng quan tâm này là một mặt phẳng. Điều đó là hiển nhiên, nhưng tốt hơn hết là nói rõ ràng!\n",
    "- Vì vậy, hãy chắc chắn rằng bạn có một hộp các tông trong tay khi bạn chọn khu vực quan tâm này. Ngoài ra, sẽ tốt hơn nếu hộp các tông có một loạt các mẫu và các điểm đặc biệt để dễ dàng phát hiện và theo dõi các điểm đặc trưng trên nó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lớp **PoseEstimator** sẽ nhận được các khu vực quan tâm từ phương thức của nó, **add_target()** và sẽ trích xuất các điểm đặc trưng đó từ chúng, cho phép chúng ta theo dõi các chuyển động của đối tượng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseEstimator(object):\n",
    "    def __init__(self):\n",
    "        # Use locality sensitive hashing algorithm\n",
    "        flann_params = dict(algorithm = 6, table_number = 6, key_size = 12, multi_probe_level = 1)\n",
    "        \n",
    "        self.min_matches = 10\n",
    "        self.cur_target = namedtuple('Current', 'image, rect, keypoints, descriptors, data')\n",
    "        self.tracked_target = namedtuple('Tracked', 'target, points_prev, points_cur, H, quad')\n",
    "                                         \n",
    "        self.feature_detector = cv2.ORB_create()\n",
    "        self.feature_detector.setMaxFeatures(1000)\n",
    "        self.feature_matcher = cv2.FlannBasedMatcher(flann_params, {})\n",
    "        self.tracking_targets = []\n",
    "                                     \n",
    "    # Function to add a new target for tracking\n",
    "    def add_target(self, image, rect, data=None):\n",
    "        x_start, y_start, x_end, y_end = rect\n",
    "        keypoints, descriptors = [], []\n",
    "        for keypoint, descriptor in zip(*self.detect_features(image)):\n",
    "            x, y = keypoint.pt\n",
    "            if x_start <= x <= x_end and y_start <= y <= y_end:\n",
    "                keypoints.append(keypoint)\n",
    "                descriptors.append(descriptor)\n",
    "        descriptors = np.array(descriptors, dtype='uint8')\n",
    "        self.feature_matcher.add([descriptors])\n",
    "        target = self.cur_target(image=image, rect=rect, keypoints=keypoints, descriptors=descriptors, data=None)\n",
    "        self.tracking_targets.append(target)\n",
    "                                     \n",
    "    # To get a list of detected objects\n",
    "    def track_target(self, frame):\n",
    "        self.cur_keypoints, self.cur_descriptors = self.detect_features(frame)\n",
    "        if len(self.cur_keypoints) < self.min_matches: return []\n",
    "        try: matches = self.feature_matcher.knnMatch(self.cur_descriptors, k=2)\n",
    "        except Exception as e:\n",
    "            print('Invalid target, please select another with features to extract')\n",
    "            return []\n",
    "                                     \n",
    "        matches = [match[0] for match in matches if len(match) == 2 and\n",
    "        match[0].distance < match[1].distance * 0.75]\n",
    "        if len(matches) < self.min_matches: return []\n",
    "                                     \n",
    "        matches_using_index = [[] for _ in range(len(self.tracking_targets))]\n",
    "        for match in matches:\n",
    "            matches_using_index[match.imgIdx].append(match)\n",
    "                                     \n",
    "        tracked = []\n",
    "        for image_index, matches in enumerate(matches_using_index):\n",
    "            if len(matches) < self.min_matches: continue\n",
    "            target = self.tracking_targets[image_index]\n",
    "            points_prev = [target.keypoints[m.trainIdx].pt for m in matches]\n",
    "                                     \n",
    "            points_cur = [self.cur_keypoints[m.queryIdx].pt for m in matches]\n",
    "            points_prev, points_cur = np.float32((points_prev, points_cur))\n",
    "            H, status = cv2.findHomography(points_prev, points_cur, cv2.RANSAC, 3.0)\n",
    "            status = status.ravel() != 0\n",
    "                                     \n",
    "            if status.sum() < self.min_matches: continue\n",
    "                                     \n",
    "            points_prev, points_cur = points_prev[status], points_cur[status]\n",
    "            x_start, y_start, x_end, y_end = target.rect\n",
    "            quad = np.float32([[x_start, y_start], [x_end, y_start], [x_end, y_end], [x_start, y_end]])\n",
    "            quad = cv2.perspectiveTransform(quad.reshape(1, -1, 2), H).reshape(-1, 2)\n",
    "            track = self.tracked_target(target=target, points_prev=points_prev, points_cur=points_cur, H=H, quad=quad)\n",
    "            tracked.append(track)\n",
    "                                     \n",
    "        tracked.sort(key = lambda x: len(x.points_prev), reverse=True)\n",
    "        return tracked\n",
    "                                     \n",
    "    # Detect features in the selected ROIs and return the keypoints and descriptors\n",
    "    def detect_features(self, frame):\n",
    "        keypoints, descriptors = self.feature_detector.detectAndCompute(frame, None)\n",
    "        if descriptors is None: descriptors = []\n",
    "        return keypoints, descriptors\n",
    "                                     \n",
    "    # Function to clear all the existing targets\n",
    "    def clear_targets(self):\n",
    "        self.feature_matcher.clear()\n",
    "        self.tracking_targets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hãy để việc theo dõi bắt đầu! Chúng ta sẽ di chuyển hộp các tông xung quanh để xem điều gì xảy ra.\n",
    "- Như bạn có thể thấy, các điểm đặc trưng đang được theo dõi bên trong khu vực quan tâm. Hãy giữ nó ở một góc và xem điều gì sẽ xảy ra.\n",
    "- Có vẻ như các điểm đặc trưng đang được theo dõi chính xác. Như chúng ta có thể thấy, hình chữ nhật được phủ lên đang thay đổi hướng của nó theo bề mặt của hộp các tông."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import namedtuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class VideoHandler(object):\n",
    "    def __init__(self, capId, scaling_factor, win_name):\n",
    "        self.cap = cv2.VideoCapture(capId)\n",
    "        self.pose_tracker = PoseEstimator()\n",
    "        self.win_name = win_name\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "        ret, frame = self.cap.read()\n",
    "        self.rect = None\n",
    "        self.frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "        self.roi_selector = ROISelector(win_name, self.frame, self.set_rect)\n",
    "        \n",
    "    def set_rect(self, rect):\n",
    "        self.rect = rect\n",
    "        self.pose_tracker.add_target(self.frame, rect)\n",
    "    def start(self):\n",
    "        paused = False\n",
    "        while True:\n",
    "            if not paused or self.frame is None:\n",
    "                ret, frame = self.cap.read()\n",
    "                scaling_factor = self.scaling_factor\n",
    "                frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "                if not ret: break\n",
    "                self.frame = frame.copy()\n",
    "                \n",
    "            img = self.frame.copy()\n",
    "            if not paused and self.rect is not None:\n",
    "                tracked = self.pose_tracker.track_target(self.frame)\n",
    "                for item in tracked:\n",
    "                    cv2.polylines(img, [np.int32(item.quad)], True, (255, 255, 255), 2)\n",
    "                    for (x, y) in np.int32(item.points_cur):\n",
    "                        cv2.circle(img, (x, y), 2, (255, 255, 255))\n",
    "                        \n",
    "            self.roi_selector.draw_rect(img, self.rect)\n",
    "            cv2.imshow(self.win_name, img)\n",
    "            ch = cv2.waitKey(1)\n",
    "            if ch == ord(' '): paused = not paused\n",
    "            if ch == ord('c'): self.pose_tracker.clear_targets()\n",
    "            if ch == 27: \n",
    "                self.cap.release()\n",
    "                break\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    VideoHandler(0, 0.8, 'Tracker').start()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened inside the code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Để bắt đầu, chúng ta có một lớp **PoseEstimator** thực hiện tất cả các công việc `\"nặng\"` ở đây.\n",
    "- Chúng ta cần một cái gì đó để phát hiện các đặc trưng trong hình ảnh và một cái gì đó để so khớp với các đặc trưng giữa các hình ảnh liên tiếp.\n",
    "- Vì vậy, chúng ta sử dụng trình phát hiện tính năng ORB và trình so khớp đặc trưng **Flann** để tìm kiếm hàng xóm gần nhất (nearest nei) nhanh nhất trong các đặc trưng được trích xuất. Như bạn có thể thấy, chúng ta khởi tạo lớp với các tham số này trong constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bất cứ khi nào chúng ta chọn một khu vực quan tâm, chúng ta gọi phương thức **add_target** để thêm vùng đó vào danh sách mục tiêu theo dõi của chúng ta.\n",
    "- Phương thức này chỉ trích xuất các đực trưng từ vùng quan tâm đó và lưu trữ chúng trong một trong các biến. Bây giờ chúng ta có mục tiêu, chúng ta đã sẵn sàng để theo dõi nó!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Phương thức **track_target** xử lý tất cả các theo dõi. Chúng ta lấy frame hiện tại và trích xuất tất cả các điểm chính (keypoints). Tuy nhiên, chúng ta không thực sự quan tâm đến tất cả các điểm chính trong khung hình hiện tại của video.\n",
    "- Chúng tôi chỉ muốn các điểm chính thuộc về đối tượng mục tiêu của chúng ta. Vì vậy, bây giờ công việc của chúng tôi là tìm các điểm chính gần nhất trong frame hiện tại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta có một tập hợp các điểm chính trong frame hiện tại và chúng ta có một tập hợp các điểm chính khác từ đối tượng mục tiêu của chúng ta trong frame trước đó.\n",
    "- Bước tiếp theo là trích xuất ma trận homography từ các điểm phù hợp này. Ma trận homography này cho chúng ta biết cách biến đổi hình chữ nhật được phủ lên sao cho nó thẳng hàng với bề mặt của hộp các tông.\n",
    "- Chúng ta chỉ cần lấy ma trận homography này và áp dụng nó cho hình chữ nhật được phủ lên để có được vị trí mới của tất cả các điểm của hộp các tông."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How to augment our reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta đã biết cách theo dõi các vật thể phẳng, hãy xem cách phủ các vật thể 3D lên trên thế giới thực. Các đối tượng là 3D nhưng video trên màn hình của chúng ta là 2D.\n",
    "- Vì vậy, bước đầu tiên ở đây là hiểu cách ánh xạ các vật thể 3D đó lên các bề mặt 2D sao cho chúng trông thật. Chúng ta chỉ cần chiếu các điểm 3D đó lên các bề mặt phẳng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Mapping coordinates from 3D to 2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khi chúng ta ước tính tư thế, chúng ta chiếu các điểm từ 3D sang 2D. Hãy xem xét hình ảnh sau đây:\n",
    "\n",
    "![](images\\example12.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Như chúng ta có thể thấy ở đây, điều khiển từ xa của TV là một vật thể 3D nhưng chúng ta đang nhìn thấy nó trên mặt phẳng 2D. Bây giờ nếu chúng ta di chuyển nó xung quanh, nó sẽ trông như thế này:\n",
    "\n",
    "![](images\\example13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đối tượng 3D này vẫn ở trên mặt phẳng 2D. Đối tượng đã di chuyển đến một vị trí khác và khoảng cách từ camera cũng thay đổi.\n",
    "- Làm thế nào để chúng ta tính toán các tọa độ? Chúng ta cần một cơ chế để ánh xạ đối tượng 3D này lên bề mặt 2D. Đây là lý do vì sao 3D-to-2D projection thực sự quan trọng.\n",
    "- Chúng ta chỉ cần ước tính vị trí máy ảnh ban đầu để bắt đầu. Bây giờ, hãy giả sử rằng các thông số nội tại của máy ảnh đã được biết đến.\n",
    "- Vì vậy, chúng ta chỉ có thể sử dụng hàm slovePnP trong OpenCV để ước tính tư thế của máy ảnh. Hàm này được sử dụng để ước tính tư thế của đối tượng bằng cách sử dụng một tập hợp các điểm như đã thấy dòng đoạn code sau. Bạn có thể đọc thêm về nó tại: https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solvePnP(InputArray objectPoints, InputArray imagePoints, InputArray cameraMatrix, InputArray distCoeffs, OutputArray rvec, OutputArray tvec, bool useExtrinsicGuess, int flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Khi chúng ta thực hiện điều này, chúng ta cần chiếu các điểm này lên mặt phẳng 2D. Chúng ta sử dụng hàm **projectPoints** để làm điều này. Hàm này tính toán các hình chiếu của các điểm 3D đó lên mặt phẳng 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 How to overlay 3D objects on a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta có tất cả các khối khác nhau, chúng ta đã sẵn sàng để xây dựng hệ thống cuối cùng. Giả sử chúng ta muốn phủ một kim tự tháp lên trên hộp các tông của chúng ta, như được hiển thị ở đây:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hãy nghiêng hộp các tông để xem điều gì xảy ra:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hình như kim tự tháp đang theo bề mặt. Hãy thêm mục tiêu thứ hai:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bạn có thể tiếp tục thêm nhiều mục tiêu hơn và tất cả những kim tự tháp đó sẽ được theo dõi riêng biệt. Chúng ta hãy xem làm thế nào để làm điều này bằng cách sử dụng OpenCV Python. Đảm bảo lưu tệp trước đó dưới dạng pose_estimation.py vì chúng ta sẽ import một vài lớp từ đó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Tracker(object):\n",
    "    def __init__(self, capId, scaling_factor, win_name):\n",
    "        self.cap = cv2.VideoCapture(capId)\n",
    "        self.rect = None\n",
    "        self.win_name = win_name\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.tracker = PoseEstimator()\n",
    "        \n",
    "        ret, frame = self.cap.read()\n",
    "        self.rect = None\n",
    "        self.frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        self.roi_selector = ROISelector(win_name, self.frame, self.set_rect)\n",
    "        self.overlay_vertices = np.float32([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0], [0.5, 0.5, 4]])\n",
    "        self.overlay_edges = [(0, 1), (1, 2), (2, 3), (3, 0), (0,4), (1,4), (2,4), (3,4)]\n",
    "        self.color_base = (0, 255, 0)\n",
    "        self.color_lines = (0, 0, 0)\n",
    "        \n",
    "    def set_rect(self, rect):\n",
    "        self.rect = rect\n",
    "        self.tracker.add_target(self.frame, rect)\n",
    "    def start(self):\n",
    "        paused = False\n",
    "        while True:\n",
    "            if not paused or self.frame is None:\n",
    "                ret, frame = self.cap.read()\n",
    "                scaling_factor = self.scaling_factor\n",
    "                frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "            if not ret: break\n",
    "                \n",
    "            self.frame = frame.copy()\n",
    "            \n",
    "            img = self.frame.copy()\n",
    "            if not paused:\n",
    "                tracked = self.tracker.track_target(self.frame)\n",
    "                for item in tracked:\n",
    "                    cv2.polylines(img, [np.int32(item.quad)], True, self.color_lines, 2)\n",
    "                    for (x, y) in np.int32(item.points_cur):\n",
    "                        cv2.circle(img, (x, y), 2, self.color_lines)\n",
    "                    self.overlay_graphics(img, item)\n",
    "                    \n",
    "            self.roi_selector.draw_rect(img, self.rect)\n",
    "            cv2.imshow(self.win_name, img)\n",
    "            ch = cv2.waitKey(1)\n",
    "            if ch == ord(' '): self.paused = not self.paused\n",
    "            if ch == ord('c'): self.tracker.clear_targets()\n",
    "            if ch == 27: \n",
    "                self.cap.release()\n",
    "                break\n",
    "            \n",
    "    def overlay_graphics(self, img, tracked):\n",
    "        x_start, y_start, x_end, y_end = tracked.target.rect\n",
    "        quad_3d = np.float32([[x_start, y_start, 0], [x_end, y_start, 0],\n",
    "        [x_end, y_end, 0], [x_start, y_end, 0]])\n",
    "        h, w = img.shape[:2]\n",
    "        K = np.float64([[w, 0, 0.5*(w-1)],\n",
    "                        [0, w, 0.5*(h-1)],\n",
    "                        [0, 0, 1.0]])\n",
    "        dist_coef = np.zeros(4)\n",
    "        ret, rvec, tvec = cv2.solvePnP(objectPoints=quad_3d, imagePoints=tracked.quad, cameraMatrix=K, distCoeffs=dist_coef)\n",
    "        verts = self.overlay_vertices * [(x_end-x_start), (y_end-y_start), -(x_end-x_start)*0.3] + (x_start, y_start, 0)\n",
    "        verts = cv2.projectPoints(verts, rvec, tvec, cameraMatrix=K, distCoeffs=dist_coef)[0].reshape(-1, 2)\n",
    "        verts_floor = np.int32(verts).reshape(-1,2)\n",
    "        \n",
    "        cv2.drawContours(img, contours=[verts_floor[:4]], contourIdx=-1, color=self.color_base, thickness=-3)\n",
    "        cv2.drawContours(img, contours=[np.vstack((verts_floor[:2], verts_floor[4:5]))], contourIdx=-1, color=(0,255,0), thickness=-3)\n",
    "        cv2.drawContours(img, contours=[np.vstack((verts_floor[1:3], verts_floor[4:5]))], contourIdx=-1, color=(255,0,0), thickness=-3)\n",
    "        cv2.drawContours(img, contours=[np.vstack((verts_floor[2:4], verts_floor[4:5]))], contourIdx=-1, color=(0,0,150), thickness=-3)\n",
    "        cv2.drawContours(img, contours=[np.vstack((verts_floor[3:4], verts_floor[0:1], verts_floor[4:5]))], contourIdx=-1, color=(255,255,0), thickness=-3)\n",
    "        for i, j in self.overlay_edges:\n",
    "            (x_start, y_start), (x_end, y_end) = verts[i], verts[j]\n",
    "            cv2.line(img, (int(x_start), int(y_start)), (int(x_end), int(y_end)), self.color_lines, 2)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    Tracker(0, 0.8, 'Augmented Reality').start()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Let's look at the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lớp **Tracker** được sử dụng để thực hiện tất cả các tính toán ở đây. Chúng tôi khởi tạo lớp với cấu trúc kim tự tháp được xác định bằng cách sử dụng các cạnh và đỉnh. Cái cách mà chúng ta sử dụng để theo dõi bề mặt giống như chúng ta đã thảo luận trước đó bởi vì chúng ta đang sử dụng cùng một lớp. Chúng tôi chỉ cần sử dụng solvePnP và projectPoints để ánh xạ kim tự tháp 3D sang bề mặt 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Let's add some movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta đã biết cách thêm một kim tự tháp ảo, hãy xem liệu chúng ta có thể thêm một số chuyển động không. Chúng ta hãy xem làm thế nào chúng ta có thể thay đổi linh hoạt chiều cao của kim tự tháp. Khi bạn bắt đầu, kim tự tháp sẽ trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu bạn chờ đợi một thời gian, kim tự tháp sẽ cao hơn và sẽ trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hãy xem cách thực hiện trong OpenCV Python. Bên trong code augmented reality mà chúng ta vừa thảo luận, hãy thêm đoạn code sau vào cuối phương thức `__init__` trong lớp Tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.overlay_vertices = np.float32([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0], [0.5, 0.5, 4]])\n",
    "#self.overlay_edges = [(0, 1), (1, 2), (2, 3), (3, 0), (0,4), (1,4), (2,4), (3,4)]\n",
    "#self.color_base = (0, 255, 0)\n",
    "#self.color_lines = (0, 0, 0)\n",
    "#self.graphics_counter = 0\n",
    "#self.time_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta có cấu trúc, chúng ta cần thêm code để tự động thay đổi chiều cao. Thay thế phương thức **overlay_graphics()** bằng phương pháp sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_graphics(self, img, tracked):\n",
    "    x_start, y_start, x_end, y_end = tracked.target.rect\n",
    "    quad_3d = np.float32([[x_start, y_start, 0], [x_end, y_start, 0], [x_end, y_end, 0], [x_start, y_end, 0]])\n",
    "    h, w = img.shape[:2]\n",
    "    K = np.float64([[w, 0, 0.5*(w-1)],\n",
    "                    [0, w, 0.5*(h-1)],\n",
    "                    [0, 0, 1.0]])\n",
    "    \n",
    "    dist_coef = np.zeros(4)\n",
    "    ret, rvec, tvec = cv2.solvePnP(objectPoints=quad_3d, imagePoints=tracked.quad,\n",
    "    cameraMatrix=K, distCoeffs=dist_coef)\n",
    "    verts = self.overlay_vertices * [(x_end-x_start), (y_end-y_start), -(x_end-x_start)*0.3] + (x_start, y_start, 0)\n",
    "    verts = cv2.projectPoints(verts, rvec, tvec, cameraMatrix=K, distCoeffs=dist_coef)[0].reshape(-1, 2)\n",
    "    verts_floor = np.int32(verts).reshape(-1,2)\n",
    "    cv2.drawContours(img, contours=[verts_floor[:4]], contourIdx=-1, color=self.color_base, thickness=-3)\n",
    "    cv2.drawContours(img, contours=[np.vstack((verts_floor[:2], verts_floor[4:5]))], contourIdx=-1, color=(0,255,0), thickness=-3)\n",
    "    cv2.drawContours(img, contours=[np.vstack((verts_floor[1:3], verts_floor[4:5]))], contourIdx=-1, color=(255,0,0), thickness=-3)\n",
    "    cv2.drawContours(img, contours=[np.vstack((verts_floor[2:4], verts_floor[4:5]))], contourIdx=-1, color=(0,0,150), thickness=-3)\n",
    "    cv2.drawContours(img, contours=[np.vstack((verts_floor[3:4], verts_floor[0:1], verts_floor[4:5]))], contourIdx=-1, color=(255,255,0),thickness=-3)\n",
    "    for i, j in self.overlay_edges:\n",
    "        (x_start, y_start), (x_end, y_end) = verts[i], verts[j]\n",
    "        cv2.line(img, (int(x_start), int(y_start)), (int(x_end), int(y_end)), self.color_lines, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bây giờ chúng ta đã biết cách thay đổi chiều cao, hãy tiếp tục và thực hiện điệu nhảy kim tự tháp cho chúng ta. Chúng ta có thể làm cho đỉnh của kim tự tháp dao động theo định kỳ. Vì vậy, khi bạn bắt đầu, nó sẽ trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nếu bạn chờ đợi một thời gian, nó sẽ trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bạn có thể xem **augmented_reality_motion.py** để biết chi tiết triển khai. Trong thử nghiệm tiếp theo của chúng ta, chúng ta sẽ làm cho toàn bộ kim tự tháp di chuyển xung quanh khu vực quan tâm. Chúng ta có thể làm cho nó di chuyển theo bất kỳ cách nào chúng ta muốn. Hãy bắt đầu bằng cách thêm chuyển động đường chéo tuyến tính xung quanh khu vực quan tâm đã chọn của chúng ta. Khi bạn bắt đầu, nó sẽ trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau một thời gian, nó sẽ trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tham khảo *augmented_reality_dANCE.py* để xem cách thay đổi phương thức **overlay_graphics()** để làm cho nó nhảy. Chúng ta hãy xem liệu chúng ta có thể làm cho kim tự tháp đi vòng tròn trong khu vực quan tâm của chúng ta không. Khi bạn bắt đầu, nó sẽ trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sau một thời gian, nó sẽ chuyển sang một vị trí mới:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bạn có thể tham khảo augmented_reality_circular_motion.py để xem cách thực hiện điều này. Bạn có thể làm cho nó làm bất cứ điều gì bạn muốn. Bạn chỉ cần đưa ra công thức toán học phù hợp và kim tự tháp sẽ nhảy theo giai điệu của bạn! Bạn cũng có thể thử các đối tượng ảo khác để xem bạn có thể làm gì với nó. Có rất nhiều thứ bạn có thể làm với rất nhiều đối tượng khác nhau. Những ví dụ này cung cấp các điểm tham chiếu tốt, trên đó bạn có thể xây dựng nhiều ứng dụng thực tế tăng cường thú vị."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong chương này, bạn đã tìm hiểu về các khái niệm về **thực tế tăng cường (concepts of augmented reality)** và có được sự hiểu biết về một hệ thống thực tế tăng cường trông như thế nào.\n",
    "- Chúng ta đã thảo luận về các biến đổi hình học cần thiết cho thực tế tăng cường. Bạn cũng đã học cách sử dụng các phép biến đổi đó để ước tính tư thế máy ảnh và bạn đã học cách theo dõi các vật thể phẳng.\n",
    "- Chúng ta đã thảo luận về cách chúng ta có thể thêm các đối tượng ảo trên đỉnh của thế giới thực. Bạn đã học cách sửa đổi các đối tượng ảo theo nhiều cách khác nhau để thêm các hiệu ứng thú vị.\n",
    "- **Trong chương tiếp theo, chúng ta sẽ tìm hiểu cách áp dụng các kỹ thuật học máy (machine learning), cùng với mạng lưới thần kinh nhân tạo (`Artificial Neural Network - ANN`), giúp chúng ta nâng cao kiến thức đã thu được trong Chương 09. Object Recognition.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
